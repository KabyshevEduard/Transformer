{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, d, att_dim):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.W_q = torch.nn.Linear(d, att_dim)\n",
    "        self.W_k = torch.nn.Linear(d, att_dim)\n",
    "        self.W_v = torch.nn.Linear(d, att_dim)\n",
    "        self.W_tr = torch.nn.Linear(att_dim, d)\n",
    "        self.feed1 = torch.nn.Linear(d, d*4)\n",
    "        self.feed2 = torch.nn.Linear(d*4, d)\n",
    "        self.norm1 = torch.nn.LayerNorm(d)\n",
    "        self.norm2 = torch.nn.LayerNorm(d)\n",
    "\n",
    "    def multi_head_att(self, x):\n",
    "        Q_q = self.W_q(x)\n",
    "        Q_k = self.W_k(x)\n",
    "        Q_v = self.W_v(x)\n",
    "        context = torch.nn.functional.softmax((Q_q @ torch.transpose(Q_k, -2, -1))/math.sqrt(self.d), dim=-1)\n",
    "        context = context @ Q_v\n",
    "        Z = self.W_tr(context)\n",
    "        return Z\n",
    "    \n",
    "    def feed_forw(self, x):\n",
    "        x = self.feed1(x)\n",
    "        y = torch.nn.functional.relu(x)\n",
    "        y = self.feed2(y)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.multi_head_att(x)\n",
    "        y = self.norm1(y + x)\n",
    "        out = self.feed_forw(y)\n",
    "        out = self.norm2(out + y)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, d, att_dim):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.W_q_self = torch.nn.Linear(d, att_dim)\n",
    "        self.W_k_self = torch.nn.Linear(d, att_dim)\n",
    "        self.W_v_self = torch.nn.Linear(d, att_dim)\n",
    "        self.W_tr_self = torch.nn.Linear(att_dim, d)\n",
    "        self.W_q_cross = torch.nn.Linear(d, att_dim)\n",
    "        self.W_k_cross = torch.nn.Linear(d, att_dim)\n",
    "        self.W_v_cross = torch.nn.Linear(d, att_dim)\n",
    "        self.W_tr_cross = torch.nn.Linear(att_dim, d)\n",
    "        self.feed1 = torch.nn.Linear(d, d*4)\n",
    "        self.feed2 = torch.nn.Linear(d*4, d)\n",
    "        self.norm1 = torch.nn.LayerNorm(d)\n",
    "        self.norm2 = torch.nn.LayerNorm(d)\n",
    "        self.norm3 = torch.nn.LayerNorm(d)\n",
    "    \n",
    "    def mask(self, row, col):\n",
    "        mask = torch.ones((row, col))\n",
    "        for i in range(mask.shape[-2]):\n",
    "            for j in range(mask.shape[-1]):\n",
    "                if i < j:\n",
    "                    mask[i, j] = 0\n",
    "        return mask\n",
    "\n",
    "    def masked_self_att(self, x):\n",
    "        Q_q = self.W_q_self(x)\n",
    "        Q_k = self.W_k_self(x)\n",
    "        Q_v = self.W_v_self(x)\n",
    "        context = (Q_q @ torch.transpose(Q_k, -2, -1))/math.sqrt(self.d)\n",
    "        masked_context = context.masked_fill(self.mask(context.shape[-2], context.shape[-1]) == 0, -math.inf)\n",
    "        masked_context = torch.nn.functional.softmax(masked_context, dim=-1) @ Q_v\n",
    "        Z = self.W_tr_self(masked_context)\n",
    "        return Z\n",
    "    \n",
    "    def cross_multi_att(self, x, enc_y):\n",
    "        Q_q = self.W_q_cross(x)\n",
    "        Q_k = self.W_k_cross(enc_y)\n",
    "        Q_v = self.W_v_cross(enc_y)\n",
    "        context = torch.nn.functional.softmax((Q_q @ torch.transpose(Q_k, -2, -1))/math.sqrt(self.d), dim=-1)\n",
    "        context = context @ Q_v\n",
    "        Z = self.W_tr_cross(context)\n",
    "        return Z\n",
    "\n",
    "    def feed_forw(self, x):\n",
    "        x = self.feed1(x)\n",
    "        y = torch.nn.functional.relu(x)\n",
    "        y = self.feed2(y)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, enc_y):   \n",
    "        y = self.masked_self_att(x)\n",
    "        y = self.norm1(x + y)\n",
    "        z = self.cross_multi_att(y, enc_y)\n",
    "        z = self.norm2(z + y)\n",
    "        z_out = self.feed_forw(z)\n",
    "        z_out = self.norm3(z_out + z)\n",
    "        return z_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, enc, dec, emb_dim, att_dim, voc_size, enc_num: int, dec_num: int):\n",
    "        '''\n",
    "        enc: encoder class\n",
    "        dec: decoder class\n",
    "        emb_dim: dimension of embeddings\n",
    "        att_dim: just parameter of model\n",
    "        voc_size: number of words in target language\n",
    "        enc_num: number of encoders in encoder\n",
    "        dec_num: number of decoders in decoder\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.encoders = [enc(emb_dim, att_dim) for _ in range(enc_num)]\n",
    "        self.encoder = torch.nn.Sequential(*self.encoders)\n",
    "        self.decoders = [dec(emb_dim, att_dim) for _ in range(dec_num)]\n",
    "        self.lin = torch.nn.Linear(emb_dim, voc_size)\n",
    "\n",
    "    def forward(self, x, x_tgt):\n",
    "        enc_out = self.encoder(x)\n",
    "        out = x_tgt\n",
    "        for decoder in self.decoders:\n",
    "            out = decoder(out, enc_out)\n",
    "        out = self.lin(out)\n",
    "        return torch.nn.functional.softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 100])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russian = torch.randn((1, 5, 10))\n",
    "english = torch.randn((1, 5, 10))\n",
    "tfr = Transformer(Encoder, Decoder, 10, 30, 100, 3, 3)\n",
    "tfr(russian, english).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
